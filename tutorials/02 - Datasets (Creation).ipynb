{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Dataset Creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialization and Slicing\n",
    "\n",
    "Here we create the dataset from a bunch of audio source files\n",
    "These files resampled and sliced into chunks, according to the parameters provided in the dataset manifest (sample rate, duration, overlap).\n",
    "\n",
    "A SQLite database is also created for the dataset, to persist any useful information. \n",
    "\n",
    "*Note: When multiprocessing, progress is not tracked in jupyter notebook, so you have to look at the console*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-08-05/21:10:12.499|15.5%|82.0%|0.25GB] The dataset directory (D:\\Jupyter\\ShowBees\\datasets\\SMALL1005) already exists.\n",
      "[2020-08-05/21:10:12.501|00.0%|82.0%|0.25GB] If you really intent to CREATE this dataset, please erase this directory first\n",
      "[2020-08-05/21:10:12.501|00.0%|82.0%|0.25GB] ### ABORTING! ###\n",
      "[2020-08-05/21:10:12.501|00.0%|82.0%|0.25GB] Retrieving existing dataset\n",
      "[2020-08-05/21:10:12.501|00.0%|82.0%|0.25GB] Dataset retrieved\n",
      "[2020-08-05/21:10:12.501|00.0%|82.0%|0.25GB] ------------------------------------------------------\n",
      "[2020-08-05/21:10:12.501|00.0%|82.0%|0.25GB] DATASET PATH          : D:\\Jupyter\\ShowBees\\datasets\\SMALL1005\n",
      "[2020-08-05/21:10:12.501|00.0%|82.0%|0.25GB] DATASET DB PATH       : D:\\Jupyter\\ShowBees\\datasets\\SMALL1005\\database.db\n",
      "[2020-08-05/21:10:12.501|00.0%|82.0%|0.25GB] DATASET SAMPLES PATH  : D:\\Jupyter\\ShowBees\\datasets\\SMALL1005\\samples\n",
      "[2020-08-05/21:10:12.501|00.0%|82.0%|0.25GB] NB SOURCE AUDIO FILES : 4\n",
      "[2020-08-05/21:10:12.501|00.0%|82.0%|0.25GB] SAMPLE RATE           : 22050\n",
      "[2020-08-05/21:10:12.501|00.0%|82.0%|0.25GB] DURATION              : 1.0\n",
      "[2020-08-05/21:10:12.501|00.0%|82.0%|0.25GB] OVERLAP               : 0.5\n",
      "[2020-08-05/21:10:12.501|00.0%|82.0%|0.25GB] NB AUDIO CHUNKS       : 4744\n",
      "[2020-08-05/21:10:12.501|00.0%|82.0%|0.25GB] ------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import warnings                            # This block prevents display of harmless warnings, but should be\n",
    "warnings.filterwarnings('ignore')          # commented out till the final version, to avoid missing \"real\" warnings \n",
    "\n",
    "import kilroy_was_here                     # Mandatory. Allow access to shared python code in the upper 'codelib' directory\n",
    "from lib.audiodataset import AudioDataset  # Class for audio dataset handling\n",
    "from lib.jupytools import iprint           # timstamped (to the ms) print with CPU and RAM consumption information\n",
    "\n",
    "# Path where to find initial annotated dataset (audio and lab files)\n",
    "SOURCE_PATH ='D:/datasets/sounds/Nolasco'\n",
    "\n",
    "# Dataset name is the master key for dataset adressing\n",
    "DATASET_NAME = 'SMALL1005'\n",
    "\n",
    "# Initialize Dataset Object. \n",
    "try:\n",
    "    #By providing a source path,we implicitly indicates that you want to CREATE the data set.\n",
    "    # Run with a pool of 4 processes\n",
    "    ds = AudioDataset(DATASET_NAME, SOURCE_PATH, nprocs=4)\n",
    "except FileExistsError:\n",
    "    # To allow rerun, we catch the exception in case the dataset was already created.\n",
    "    # Ideally, you should create the dataset once for all in a dedicated notebook,\n",
    "    # and then retrieve it from other notebooks when needed\n",
    "    # Here, by not providing a source path, we implicitly express the intent of RETRIEVING\n",
    "    # an existing dataset rather than CREATING a new one\n",
    "    iprint(\"Retrieving existing dataset\")\n",
    "    ds = AudioDataset(DATASET_NAME)\n",
    "    iprint(\"Dataset retrieved\")\n",
    "    \n",
    "# The following line provides some information about the newly created (or retrived) AudioDataset object    \n",
    "ds.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add Labels\n",
    "\n",
    "Here we add **labels** to our dataset samples. Labels can be set using various functions called ***Labelizers*** which basically define the source of the label, and the way it wil be inserted into the database. Labelizers then make use of **transformers**, which (as you may have guessed) transform the source into an acceptable format.\n",
    "\n",
    "Both labelizers and transformers are either builtin within the toolbox, or developped by the user (the toolbox provides utilities functions for their development).\n",
    "\n",
    "Labels have a name, a numeric value, and an optionnal strength. \n",
    "\n",
    "- *Note1: By design, labels do not have a string value, as usually machine learning frameworks expect numerals.* \n",
    "- *Note2: Currently, labels are aimed only at binary classifiers, so their value is either 0 or 1 (no multi-class).*\n",
    "\n",
    "\n",
    "We are adding two labels, using two different labelizers:\n",
    "\n",
    "- the \"queen\" label, using the builtin FromFileName labelizer, associated with the builtin StringMatcher transformer\n",
    "- the \"nobee\" label, using the builtin FromAnnotation labelizer, without transformation\n",
    "\n",
    "\n",
    "*Note: The `setLabel` method use an \"insert or update\" based on the (sample_id, label_name) unique index. Consecutive invocations for a given label, with the same parameters won't affect the already labeled samples. Conversely, iF different parameters are used, existing samples will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-08-05/21:10:19.062|07.5%|81.2%|0.26GB] 4744 samples where processed for 'queen' label\n",
      "[2020-08-05/21:10:19.062|00.0%|81.2%|0.26GB] [1] Hive1_12_06_2018_QueenBee_H1_audio___15_00_00.wav\n",
      "[2020-08-05/21:10:19.305|17.4%|81.2%|0.26GB] [2] Hive1_31_05_2018_NO_QueenBee_H1_audio___15_00_00.wav\n",
      "[2020-08-05/21:10:19.604|14.7%|81.2%|0.26GB] [3] Hive3_15_07_2017_NO_QueenBee_H3_audio___06_10_00.wav\n",
      "[2020-08-05/21:10:19.832|12.1%|81.2%|0.26GB] [4] Hive3_20_07_2017_QueenBee_H3_audio___06_10_00.wav\n",
      "[2020-08-05/21:10:20.096|16.8%|81.2%|0.26GB] 4744 samples where processed for 'nobee' label\n"
     ]
    }
   ],
   "source": [
    "from lib import labelizers\n",
    "from lib import transformers\n",
    "\n",
    "# The \"queen\" label value is deduced from the source file name, using a StringMapper transformer\n",
    "# This transformer iterates over a list 2-uples (regular expression, target value) and return\n",
    "# the target value as soon as a match is found. Thus, you must order your list from stricter to looser\n",
    "trsfrm_queen = transformers.StringMapper(\n",
    "        [('(?i)active', 1), \n",
    "         ('(?i)missing queen', 0),\n",
    "         ('NO_QueenBee', 0),\n",
    "         ('QueenBee', 1)     \n",
    "        ])\n",
    "\n",
    "# The transformer is then used over the source filenames, using the FromFileName labelizer\n",
    "# This labelizer does not provide label strength.\n",
    "\n",
    "n = ds.setLabel('queen', labelizers.FromFileName(trsfrm_queen))\n",
    "iprint(n, \"samples where processed for 'queen' label\")\n",
    "\n",
    "\n",
    "# The \"nobee\" label value comes from annotation files, (.lab files using the same base name as the audio\n",
    "# source file they annotate), using the FromAnnotation labelizer, with no transformation.\n",
    "# This labelizer takes 3 arguments:\n",
    "# - a mandatory source path, pointing to the directory where the .lab files reside\n",
    "# - an optional Unitary THreshold, allowing to disregard any \"label\" event with a duration under this treshold\n",
    "# - an optionnal Global THreshold, allowing to affect the target label only if its strength is above this threshold\n",
    "# In other words:\n",
    "# - The label strength over a sample is computed by summing the duration of \"label\" events (if > uth) and dividing\n",
    "#   this sum by the sample duration\n",
    "# - The label value for a sample will be 1 if the label strength is above the global threshold, else 0\n",
    " \n",
    "n = ds.setLabel('nobee', labelizers.FromAnnotation(SOURCE_PATH, uth=0, gth=0))\n",
    "iprint(n, \"samples where processed for 'nobee' label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Add Attributes\n",
    "\n",
    "Here we add **attributes** to our dataset samples. Attributes can be set using various functions called ***Attributors***, either builtin within the toolbox, or developped by the user (the toolbox provides utilities functions for Attributors development).\n",
    "\n",
    "Attributes can be used to \"tag\" samples, for future subsets extractions. They have a name, and a value, always stored as a string (note the difference with labels)\n",
    "\n",
    "Here we tag each sample with the hive it belongs to. As the hive is encoded in the first 5 characters of the source file name, we use a FromFileName attributor, with a StringMatcher transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4744"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib import attributors\n",
    "\n",
    "#The string matcher transformer behave differently than the StringMapper. It uses regexp\n",
    "# capture group to retrieve part pf a string matching a specific pattern. This can be used\n",
    "# either for complex or very basic matching. Here we just ask for the five first chars,\n",
    "# provided they belong to characters valid for identifiers (A-Z, a-z,0-9 and underscore)\n",
    "ds.setAttr('hive', attributors.FromFileName(transformers.StringMatcher(\"^(\\w{5})\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Next Steps\n",
    "\n",
    "We can also add **features** and **augmentations** to our dataset samples. Features (resp. augmentations) can be set using various functions called ***Featurizers*** (resp. ***Augmentators**). Both are either builtin within the toolbox, or developped by the user (the toolbox provides utilities functions for Featurizers and Augmentators development)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
