CONSTITUTION DES CHUNKS

La labelisation bee/nobee d'un segment se base sur un agrégat de la durée des sons extérieurs ne prenant en compte que les perturbations dont la durée unitaire est supérieure à un seuil passé en paramètre.

Si cet agrégat est non nul, le segment est considéré "nobee".

Cependant, de la façon dont c'est calculé (sur la longueur de l'évenement nobee, sans considérer son recouvrement avec un segment, on rejette trop de segments. Par exemple,avec un seuil à 0.5s

Sur des segments courts, cela a peu d'importance, mais sur des segments longs, il vaudrait sans doute mieux décider en se basant sur un % de perturbation total (durée cumulée de tous les nobee d'un segment quelle que soit leur durée / durée du segment)

---------------------------------------------------------------------------------------------------------------------



Sur mon laptop (Core i7, mais seulement 8Go RAM), le chargmement du dataset de l'article met plus de 4h30 (dont 4h rien que pour le gros mp3 du dataset) pour traiter 48 fichiers et constituer les 24816 chunks de 1s.

La librairie utilisée (Librosa) pour le chargement des fichiers sons étant peu performante, on pourrait envisager de basculer sur une librairie plus performante (pydub, pySox)... Mais l'erreur est en réalité conceptuelle: en effet, on invoque en boucle librosa.core.load sur un même fichier pour chaque chunk de 1s, avec un temps d'exécution unitaire en O(n) (puisqu'il faut décoder les k premières secondes du fichier pour découper le chunk k+1. Résultat des courses, un temps d'éxecution en O(n^2) pour le slicing d'un seul fichier source...

C'était sans doute peu visible sur des chunks de plus longue durée, mais c'est rédhibitoire sur des chunks de 1s


---------------------------------------------------------------------------------------------------------------------
GENERAL
 - Passage de os.path à pathlib
 - Refactoring du code et renommages des fichiers, fonctions et variables

PREPROCESSING
 - Amélioration massive des performances (x15) grâce au chargement et resampling du fichier en une seule fois, puis constitution des chunks à partir de l'array 1D obtenu.
 - Introduction du concept de "manifest" défnissant de façon univoque un dataset
 - Ajout d'un paramètre overlap, permettant de générer des chunks en chevauchement
 - Uniformisation du renommage et découpage fonctionnel des fonctions de préprocessing, permettant leur invocation indépendante.
 - ajout d'un filtre sur le pourcentage de perturbation d'un segment par un bruit externe pour décider de son éligibilité à la labelisation de plus haut niveau (queen swarm, etc)
 - le numéro de chunck est paddé à gauche avec des zéros (pourl'équivalence des tris alphabétiques et chronologiques)
 - Si un chunk n'atteint pas la durée spécifiée (cas potentiel du dernier chunk d'un fichier source), il est n'est tout simplement pas généré. La complétion de ce type de chunk par mirroring a en effet peu d'intérêt pour des durées courtes (où l'on dispose de suffisament de fichiers) , et elle introduit un artefact non maitrisé dans le jeu de test. Si besoin, cela pourra toujour être rajouté ultérieurement.
 - suppression de la gestion partielle des exceptions: l'environnement est suffisament maitrisé pour qu'on puisse se permettre de juste interrompre le traitement
 - a contrario, ajout systématique d'asserts de sanity checks

     
 